"""
Advanced Music Analyzer
BPM, Key, Chord detection, Genre classification, Note Transcription
"""
import librosa
import numpy as np
from typing import Dict, List, Tuple, Optional
import logging
import crepe
from pathlib import Path

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MusicAnalyzer:
    """Profesyonel mÃ¼zik analiz motoru"""

    def __init__(self, model_manager=None):
        self.model_manager = model_manager
        self.key_map = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']

        # Nota isimleri (TÃ¼rkÃ§e)
        self.note_names_turkish = {
            'C': 'Do', 'C#': 'Do#', 'Db': 'Reb',
            'D': 'Re', 'D#': 'Re#', 'Eb': 'Mib',
            'E': 'Mi',
            'F': 'Fa', 'F#': 'Fa#', 'Gb': 'Solb',
            'G': 'Sol', 'G#': 'Sol#', 'Ab': 'Lab',
            'A': 'La', 'A#': 'La#', 'Bb': 'Sib',
            'B': 'Si'
        }

    def get_turkish_note_name(self, note_name: str) -> str:
        """Ä°ngilizce nota ismini TÃ¼rkÃ§e'ye Ã§evir (C4 -> Do4)"""
        # Nota ismini ayrÄ±ÅŸtÄ±r (Ã¶rn: C#4 -> C#, 4)
        note_base = ''.join([c for c in note_name if not c.isdigit()])
        octave = ''.join([c for c in note_name if c.isdigit()])

        turkish = self.note_names_turkish.get(note_base, note_base)
        return f"{turkish}{octave}" if octave else turkish

    def analyze_full(self, audio_path: str) -> Dict:
        """
        Tam mÃ¼zik analizi
        Returns: {
            'bpm': float,
            'key': str,
            'scale': str (major/minor),
            'tempo_confidence': float,
            'beats': list,
            'duration': float,
            'energy': float
        }
        """
        logger.info(f"ðŸŽ¼ MÃ¼zik analiz ediliyor: {audio_path}")

        try:
            # Audio yÃ¼kle
            y, sr = librosa.load(audio_path, sr=22050)
            duration = librosa.get_duration(y=y, sr=sr)

            analysis = {
                'duration': duration,
                'sample_rate': sr
            }

            # BPM tespiti
            bpm_data = self.detect_bpm(y, sr)
            analysis.update(bpm_data)

            # Key detection
            key_data = self.detect_key(y, sr)
            analysis.update(key_data)

            # Energy analysis
            energy = self.calculate_energy(y)
            analysis['energy'] = energy

            # Spectral features
            spectral = self.analyze_spectral_features(y, sr)
            analysis['spectral'] = spectral

            logger.info(f"âœ“ Analiz tamamlandÄ±: BPM={analysis.get('bpm', 'N/A'):.1f}, Key={analysis.get('key', 'N/A')}")
            return analysis

        except Exception as e:
            logger.error(f"Analiz hatasÄ±: {e}")
            return {'error': str(e)}

    def detect_bpm(self, y: np.ndarray, sr: int) -> Dict:
        """
        BPM (Tempo) tespiti
        """
        try:
            logger.info("  ðŸ¥ BPM tespit ediliyor...")

            # Onset strength envelope
            onset_env = librosa.onset.onset_strength(y=y, sr=sr)

            # Tempo estimation
            tempo, beats = librosa.beat.beat_track(onset_envelope=onset_env, sr=sr)

            # Beat times
            beat_times = librosa.frames_to_time(beats, sr=sr)

            # Dynamic tempo
            dtempo = librosa.beat.tempo(onset_envelope=onset_env, sr=sr, aggregate=None)

            logger.info(f"    âœ“ BPM: {tempo:.1f}")

            return {
                'bpm': float(tempo),
                'beats': beat_times.tolist(),
                'beat_count': len(beats),
                'tempo_stability': float(np.std(dtempo))
            }

        except Exception as e:
            logger.error(f"BPM tespit hatasÄ±: {e}")
            return {'bpm': 0, 'beats': [], 'beat_count': 0}

    def detect_key(self, y: np.ndarray, sr: int) -> Dict:
        """
        Anahtar (Key) tespiti - Krumhansl-Schmuckler algoritmasÄ±
        """
        try:
            logger.info("  ðŸŽ¹ Anahtar tespit ediliyor...")

            # Chromagram
            chroma = librosa.feature.chroma_cqt(y=y, sr=sr)

            # Her chroma iÃ§in ortalama enerji
            chroma_vals = np.mean(chroma, axis=1)

            # Krumhansl-Schmuckler major ve minor profilleri
            major_profile = np.array([6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88])
            minor_profile = np.array([6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17])

            # Her anahtar iÃ§in korelasyon hesapla
            major_correlations = []
            minor_correlations = []

            for i in range(12):
                # Rotate chroma to match key
                rotated = np.roll(chroma_vals, -i)

                major_corr = np.corrcoef(rotated, major_profile)[0, 1]
                minor_corr = np.corrcoef(rotated, minor_profile)[0, 1]

                major_correlations.append(major_corr)
                minor_correlations.append(minor_corr)

            # En yÃ¼ksek korelasyonu bul
            max_major = max(major_correlations)
            max_minor = max(minor_correlations)

            if max_major > max_minor:
                key_idx = major_correlations.index(max_major)
                scale = 'major'
                confidence = max_major
            else:
                key_idx = minor_correlations.index(max_minor)
                scale = 'minor'
                confidence = max_minor

            key = self.key_map[key_idx]

            logger.info(f"    âœ“ Anahtar: {key} {scale} (confidence: {confidence:.2f})")

            return {
                'key': key,
                'scale': scale,
                'key_confidence': float(confidence)
            }

        except Exception as e:
            logger.error(f"Key tespit hatasÄ±: {e}")
            return {'key': 'Unknown', 'scale': 'unknown', 'key_confidence': 0.0}

    def detect_chords(self, audio_path: str, hop_length: int = 512) -> List[Tuple[float, str]]:
        """
        Chord (Akor) tespiti
        Returns: [(time, chord_name), ...]
        """
        try:
            logger.info("  ðŸŽ¸ Akorlar tespit ediliyor...")

            y, sr = librosa.load(audio_path, sr=22050)

            # Chromagram
            chroma = librosa.feature.chroma_cqt(y=y, sr=sr, hop_length=hop_length)

            # Basit chord detection (template matching)
            chord_templates = self._get_chord_templates()

            chords = []
            times = librosa.frames_to_time(np.arange(chroma.shape[1]), sr=sr, hop_length=hop_length)

            for i in range(chroma.shape[1]):
                frame = chroma[:, i]

                # En yakÄ±n chord template'i bul
                best_chord = 'N'
                best_score = 0

                for chord_name, template in chord_templates.items():
                    score = np.dot(frame, template)
                    if score > best_score:
                        best_score = score
                        best_chord = chord_name

                chords.append((float(times[i]), best_chord))

            # ArdÄ±ÅŸÄ±k aynÄ± chordlarÄ± birleÅŸtir
            merged = []
            prev_chord = None
            for time, chord in chords:
                if chord != prev_chord:
                    merged.append((time, chord))
                    prev_chord = chord

            logger.info(f"    âœ“ {len(merged)} akor tespit edildi")
            return merged

        except Exception as e:
            logger.error(f"Chord tespit hatasÄ±: {e}")
            return []

    def _get_chord_templates(self) -> Dict[str, np.ndarray]:
        """Temel chord template'leri"""
        templates = {}

        # Major chords
        for i, root in enumerate(self.key_map):
            template = np.zeros(12)
            # Root, major third, fifth
            template[i] = 1.0
            template[(i + 4) % 12] = 0.8
            template[(i + 7) % 12] = 0.6
            templates[root] = template

            # Minor chords
            template_m = np.zeros(12)
            template_m[i] = 1.0
            template_m[(i + 3) % 12] = 0.8  # Minor third
            template_m[(i + 7) % 12] = 0.6
            templates[f"{root}m"] = template_m

        return templates

    def calculate_energy(self, y: np.ndarray) -> float:
        """Ses enerjisi hesapla (0-1 arasÄ± normalize)"""
        rms = librosa.feature.rms(y=y)[0]
        mean_energy = np.mean(rms)
        # Normalize to 0-1
        return min(1.0, float(mean_energy * 10))

    def analyze_spectral_features(self, y: np.ndarray, sr: int) -> Dict:
        """Spectral Ã¶zellikler analizi"""
        try:
            # Spectral centroid (brightness)
            centroid = librosa.feature.spectral_centroid(y=y, sr=sr)
            centroid_mean = float(np.mean(centroid))

            # Spectral rolloff
            rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)
            rolloff_mean = float(np.mean(rolloff))

            # Zero crossing rate
            zcr = librosa.feature.zero_crossing_rate(y)
            zcr_mean = float(np.mean(zcr))

            return {
                'brightness': centroid_mean,
                'rolloff': rolloff_mean,
                'zcr': zcr_mean
            }

        except Exception as e:
            logger.error(f"Spectral analiz hatasÄ±: {e}")
            return {}

    def transcribe_notes(self, audio_path: str, output_dir: str = "transcriptions") -> Dict:
        """
        ÅžarkÄ±daki tÃ¼m notalarÄ± Ã§Ä±karÄ±r (MIDI formatÄ±nda)
        Spotify'Ä±n Basic Pitch modeli kullanÄ±lÄ±r - en doÄŸru sonuÃ§

        Returns:
            {
                'midi_path': str,  # MIDI dosya yolu
                'notes': List[Dict],  # [{'note': 'C4', 'start': 0.5, 'end': 1.2, 'velocity': 80}]
                'note_count': int,
                'duration': float
            }
        """
        try:
            from basic_pitch.inference import predict
            from basic_pitch import ICASSP_2022_MODEL_PATH
            import pretty_midi

            logger.info("ðŸŽ¹ Notalar Ã§Ä±karÄ±lÄ±yor (Basic Pitch)...")

            # Ã‡Ä±kÄ±ÅŸ klasÃ¶rÃ¼ oluÅŸtur
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)

            # MIDI dosya yolu
            file_stem = Path(audio_path).stem
            midi_path = output_path / f"{file_stem}_notes.mid"

            # Basic Pitch ile transcription (EN YÃœKSEK KALITE AYARLARI)
            model_output, midi_data, note_events = predict(
                audio_path,
                ICASSP_2022_MODEL_PATH,
                onset_threshold=0.5,      # Daha hassas nota baÅŸlangÄ±cÄ± (0-1, default: 0.5)
                frame_threshold=0.3,      # Daha hassas nota tespiti (0-1, default: 0.3)
                minimum_note_length=127.70,  # Minimum nota sÃ¼resi (ms, default: 127.70)
                minimum_frequency=None,   # Minimum frekans (Hz, None = tÃ¼m notalar)
                maximum_frequency=None,   # Maximum frekans (Hz, None = tÃ¼m notalar)
                multiple_pitch_bends=True,  # Birden fazla pitch bend (daha detaylÄ±)
                melodia_trick=True        # Melodia trick (daha iyi melodi tespiti)
            )

            # MIDI kaydet
            midi_data.write(str(midi_path))

            # NotalarÄ± parse et
            notes_list = []
            for instrument in midi_data.instruments:
                for note in instrument.notes:
                    note_name = pretty_midi.note_number_to_name(note.pitch)
                    note_turkish = self.get_turkish_note_name(note_name)
                    notes_list.append({
                        'note': note_name,
                        'note_turkish': note_turkish,
                        'pitch': note.pitch,
                        'start': note.start,
                        'end': note.end,
                        'duration': note.end - note.start,
                        'velocity': note.velocity
                    })

            # Ä°statistikler
            note_count = len(notes_list)
            duration = max([n['end'] for n in notes_list]) if notes_list else 0

            # En Ã§ok kullanÄ±lan notalar
            note_freq = {}
            for n in notes_list:
                note_freq[n['note']] = note_freq.get(n['note'], 0) + 1

            top_notes = sorted(note_freq.items(), key=lambda x: x[1], reverse=True)[:10]

            logger.info(f"âœ“ {note_count} nota bulundu")
            logger.info(f"âœ“ MIDI kaydedildi: {midi_path}")

            return {
                'success': True,
                'midi_path': str(midi_path),
                'notes': notes_list,
                'note_count': note_count,
                'duration': duration,
                'top_notes': top_notes,
                'average_duration': np.mean([n['duration'] for n in notes_list]) if notes_list else 0
            }

        except Exception as e:
            logger.error(f"Nota Ã§Ä±karma hatasÄ±: {e}")
            return {
                'success': False,
                'error': str(e)
            }

    def transcribe_lyrics(self, audio_path: str, output_dir: str = "lyrics", language: str = "auto") -> Dict:
        """
        ÅžarkÄ± sÃ¶zlerini Ã§Ä±kar (Whisper AI ile)
        OpenAI'Ä±n Whisper modeli - en doÄŸru konuÅŸma tanÄ±ma

        Args:
            audio_path: Ses dosyasÄ± yolu
            output_dir: Ã‡Ä±kÄ±ÅŸ klasÃ¶rÃ¼
            language: Dil seÃ§imi ("tr", "en", "auto")

        Returns:
            {
                'success': bool,
                'lyrics': str,  # Tam ÅŸarkÄ± sÃ¶zleri
                'lyrics_timestamped': List[Dict],  # Zaman damgalÄ± sÃ¶zler
                'language': str,  # Tespit edilen dil
                'text_file': str  # Kaydedilen metin dosyasÄ±
            }
        """
        try:
            import whisper

            logger.info("ðŸŽ¤ ÅžarkÄ± sÃ¶zleri Ã§Ä±karÄ±lÄ±yor (Whisper large-v3)...")
            logger.info("â³ Model yÃ¼kleniyor (ilk kullanÄ±mda biraz sÃ¼rer)...")

            # Whisper modelini yÃ¼kle (large-v3 = EN YÃœKSEK KALITE)
            # RTX 5090 OPTIMIZATION: FP16 + device=cuda
            model = whisper.load_model("large-v3", device="cuda", download_root=None)

            # NOT: model.half() KULLANMA - Whisper transcribe() fp16=True parametresi ile hallediliyor
            # Manual half() Ã§aÄŸrÄ±sÄ± dtype uyumsuzluÄŸuna neden oluyor

            logger.info(f"ðŸŽµ Ses analiz ediliyor (Dil: {language.upper()})...")

            # HIZLI MOD: Direkt transcribe et (vokal ayÄ±rma Ã§ok uzun sÃ¼rÃ¼yor)
            vocals_path = audio_path

            # Dil ayarÄ±nÄ± belirle
            whisper_language = None  # Otomatik

            if language == "tr":
                whisper_language = "tr"
                logger.info("  ðŸ‡¹ðŸ‡· TÃ¼rkÃ§e dil zorlamasÄ± aktif")
            elif language == "en":
                whisper_language = "en"
                logger.info("  ðŸ‡¬ðŸ‡§ Ä°ngilizce dil zorlamasÄ± aktif")
            else:
                logger.info("  ðŸŒ Otomatik dil tespiti aktif")

            # Transcribe et - DÄ°L SEÃ‡Ä°MÄ°NE GÃ–RE (RTX 5090 FULL POWER)
            # NOT: initial_prompt KULLANMA - Whisper bunu ÅŸarkÄ± sÃ¶zÃ¼ sanÄ±p tekrar ediyor!
            result = model.transcribe(
                vocals_path,
                language=whisper_language,  # KullanÄ±cÄ± seÃ§imine gÃ¶re
                task="transcribe",
                verbose=False,  # Log kalabalÄ±ÄŸÄ±nÄ± azalt
                word_timestamps=True,
                condition_on_previous_text=False,  # TekrarlarÄ± engellemek iÃ§in False
                temperature=0.0,
                no_speech_threshold=0.6,  # MÃ¼zik kÄ±smÄ±nÄ± atla
                logprob_threshold=-1.0,  # DÃ¼ÅŸÃ¼k kaliteli tespitleri atla
                compression_ratio_threshold=2.4,  # TekrarlarÄ± engelle
                beam_size=10,  # 5 -> 10 (daha iyi kalite, RTX 5090 iÃ§in)
                best_of=10,  # Beam search - en iyi sonuÃ§larÄ± seÃ§
                fp16=True  # FP16 precision - RTX 5090'da 2x hÄ±zlÄ±
            )

            # SonuÃ§lar
            lyrics_full = result['text']
            language = result.get('language', 'unknown')

            # Zaman damgalÄ± sÃ¶zler (kelime kelime)
            lyrics_timestamped = []
            for segment in result['segments']:
                lyrics_timestamped.append({
                    'start': segment['start'],
                    'end': segment['end'],
                    'text': segment['text'].strip()
                })

            # Ã‡Ä±kÄ±ÅŸ klasÃ¶rÃ¼
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)

            # Metin dosyasÄ±na kaydet
            file_stem = Path(audio_path).stem
            lyrics_file = output_path / f"{file_stem}_lyrics.txt"

            with open(lyrics_file, 'w', encoding='utf-8') as f:
                f.write(f"ðŸŽµ ÅžARKI SÃ–ZLERÄ°: {file_stem}\n")
                f.write(f"{'='*60}\n\n")
                f.write(lyrics_full)
                f.write(f"\n\n{'='*60}\n")
                f.write("ðŸ“ ZAMAN DAMGALI SÃ–ZLER:\n")
                f.write(f"{'='*60}\n\n")
                for item in lyrics_timestamped:
                    min_start = int(item['start'] // 60)
                    sec_start = int(item['start'] % 60)
                    f.write(f"[{min_start:02d}:{sec_start:02d}] {item['text']}\n")

            logger.info(f"âœ“ ÅžarkÄ± sÃ¶zleri Ã§Ä±karÄ±ldÄ±")
            logger.info(f"âœ“ Dil: {language}")
            logger.info(f"âœ“ Metin dosyasÄ±: {lyrics_file}")

            return {
                'success': True,
                'lyrics': lyrics_full,
                'lyrics_timestamped': lyrics_timestamped,
                'language': language,
                'text_file': str(lyrics_file),
                'word_count': len(lyrics_full.split())
            }

        except Exception as e:
            logger.error(f"ÅžarkÄ± sÃ¶zÃ¼ Ã§Ä±karma hatasÄ±: {e}")
            return {
                'success': False,
                'error': str(e)
            }

    def detect_pitch_contour(self, audio_path: str) -> Dict:
        """
        Pitch contour tespiti (vokal analizi iÃ§in)
        CREPE kullanarak yÃ¼ksek doÄŸrulukta pitch detection
        """
        try:
            logger.info("  ðŸŽ¤ Pitch contour tespit ediliyor (CREPE)...")

            y, sr = librosa.load(audio_path, sr=16000)  # CREPE 16kHz ister

            # CREPE ile pitch detection
            time, frequency, confidence, activation = crepe.predict(
                y,
                sr,
                viterbi=True,
                model_capacity='tiny'  # 'tiny', 'small', 'medium', 'large', 'full'
            )

            # Sadece yÃ¼ksek confidence olanlarÄ± al
            valid_indices = confidence > 0.5
            valid_times = time[valid_indices]
            valid_freqs = frequency[valid_indices]
            valid_conf = confidence[valid_indices]

            logger.info(f"    âœ“ {len(valid_freqs)} pitch noktasÄ± tespit edildi")

            return {
                'times': valid_times.tolist(),
                'frequencies': valid_freqs.tolist(),
                'confidence': valid_conf.tolist(),
                'mean_pitch_hz': float(np.mean(valid_freqs)) if len(valid_freqs) > 0 else 0,
                'pitch_range': float(np.ptp(valid_freqs)) if len(valid_freqs) > 0 else 0
            }

        except Exception as e:
            logger.error(f"Pitch contour hatasÄ±: {e}")
            return {}

    def suggest_optimal_pitch_shift(self, audio_path: str, target_key: str = 'C') -> float:
        """
        AI Ã¶neri: Bu ÅŸarkÄ± iÃ§in optimal pitch shift deÄŸeri
        """
        try:
            logger.info(f"  ðŸ¤– Optimal pitch shift hesaplanÄ±yor (target: {target_key})...")

            # Mevcut key'i tespit et
            y, sr = librosa.load(audio_path, sr=22050)
            key_data = self.detect_key(y, sr)
            current_key = key_data['key']

            # Key'ler arasÄ± farkÄ± hesapla
            current_idx = self.key_map.index(current_key)
            target_idx = self.key_map.index(target_key.upper())

            # En kÄ±sa yolu bul (circular)
            diff = target_idx - current_idx
            if abs(diff) > 6:
                diff = diff - 12 * np.sign(diff)

            logger.info(f"    âœ“ Ã–neri: {current_key} â†’ {target_key} = {diff:+d} semitone")

            return float(diff)

        except Exception as e:
            logger.error(f"Optimal pitch shift hatasÄ±: {e}")
            return 0.0


if __name__ == "__main__":
    # Test
    analyzer = MusicAnalyzer()

    # Test ile basit bir audio oluÅŸtur
    print("Music Analyzer hazÄ±r!")
    print("KullanÄ±m: analyzer.analyze_full('song.mp3')")
